"""Utility functions for K-means benchmarking."""

import csv
import json
import numpy as np
import os
import time
from pathlib import Path

# Path constants for benchmark data
BENCHMARKS_ROOT = Path(__file__).parent
DATA_DIR = BENCHMARKS_ROOT / 'data'
GROUND_TRUTH_DIR = BENCHMARKS_ROOT / 'ground_truth'


def get_data_path(dataset):
    """Get the path to a data file.

    Args:
        dataset: Dataset name (e.g., "openai", "mxbai")

    Returns:
        Path to the data file
    """
    return DATA_DIR / f'data_{dataset}.bin'


def get_query_path(dataset):
    """Get the path to a query data file.

    Args:
        dataset: Dataset name (e.g., "openai", "mxbai")

    Returns:
        Path to the query data file
    """
    return DATA_DIR / f'data_{dataset}_test.bin'


def get_ground_truth_path(dataset):
    """Get the path to a ground truth file.

    Args:
        dataset: Dataset name (e.g., "openai", "mxbai")

    Returns:
        Path to the ground truth file
    """
    return GROUND_TRUTH_DIR / f'{dataset}.json'


# Dataset configurations: name -> (num_vectors, num_dimensions)
DATASET_PARAMS = {
    "mxbai": (769_382, 1024),
    "openai": (999_000, 1536),
    "wiki": (260_372, 3072),
    "arxiv": (2_253_000, 768),
    "sift": (1_000_000, 128),
    "fmnist": (60_000, 784),
    "glove200": (1_183_514, 200),
    "glove100": (1_183_514, 100),
    "glove50": (1_183_514, 50),
    "gist": (1_000_000, 960),
    "contriever": (990_000, 768),
    "clip": (1_281_167, 512),
    "yahoo": (677_305, 384),
    "llama": (256_921, 128),
    "yi": (187_843, 128),
    "yandex": (1_000_000, 200),
    "cohere": (35_166_920, 768)
}

# Standard exploration fractions for recall computation
EXPLORE_FRACTIONS = [
    0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009,
    0.0100, 0.0125, 0.0150, 0.0175, 0.0200, 0.0225, 0.0250, 0.0275,
    0.0300, 0.0325, 0.0350, 0.0375, 0.0400, 0.0425, 0.0450, 0.0475, 0.0500,
    0.1
]

# KNN values to test
KNN_VALUES = [10, 100]

# Benchmark configuration
MAX_ITERS = 25
N_QUERIES = 1000

# Early termination benchmark configuration
RECALL_TOL_VALUES = [0.01, 0.005, 0.001, 0.0005, 0.0001]
FAISS_EARLY_TERM_ITERS = [10, 25]
SCIKIT_EARLY_TERM_MAX_ITERS = 300
SCIKIT_EARLY_TERM_TOL = 1e-8

# Sampling fraction values for sampling experiment
SAMPLING_FRACTION_VALUES = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]

# Iteration values for pareto experiment (grid search)
PARETO_ITERS_VALUES = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]

# n_clusters values for varying_k experiment
VARYING_K_VALUES = [10, 100, 1000, 10000, 100000]


def load_ground_truth(filename):
    """Load ground truth from JSON file.

    Args:
        filename: Path to JSON file containing ground truth

    Returns:
        Dictionary mapping query index (as string) to list of vector IDs
    """
    with open(filename, 'r') as f:
        return json.load(f)


def compute_recall(gt_dict, assignments, queries, centroids, num_centroids, knn):
    """Compute recall@K for different exploration fractions.

    Args:
        gt_dict: Ground truth dictionary (query_idx -> vector_ids)
        assignments: Cluster assignments for all data vectors
        queries: Query vectors (n_queries, d)
        centroids: Centroid vectors (num_centroids, d)
        num_centroids: Number of centroids
        knn: Number of ground truth neighbors to consider

    Returns:
        List of tuples (centroids_to_explore, explore_fraction, recall_mean, recall_std, avg_vectors_to_visit)
    """
    n_queries = queries.shape[0]

    # Count cluster sizes to compute vectors to visit
    cluster_sizes = np.bincount(assignments, minlength=num_centroids)

    # Compute distances from queries to centroids
    # Using L2 distance: ||q - c||^2 = ||q||^2 + ||c||^2 - 2*q·c
    query_norms = np.sum(queries ** 2, axis=1, keepdims=True)  # (n_queries, 1)
    centroid_norms = np.sum(centroids ** 2, axis=1, keepdims=True).T  # (1, num_centroids)
    dot_products = queries @ centroids.T  # (n_queries, num_centroids)
    distances = query_norms + centroid_norms - 2 * dot_products  # (n_queries, num_centroids)

    results = []
    for explore_frac in EXPLORE_FRACTIONS:
        centroids_to_explore = max(1, int(num_centroids * explore_frac))

        # Find top-N nearest centroids for each query
        top_centroid_indices = np.argsort(distances, axis=1)[:, :centroids_to_explore]

        # Compute recall and vectors to visit for each query
        query_recalls = []
        total_vectors_to_visit = 0
        for query_idx in range(n_queries):
            query_key = str(query_idx)
            if query_key not in gt_dict:
                continue

            gt_vector_ids = gt_dict[query_key][:knn]  # List of ground truth vector IDs
            top_centroids_list = top_centroid_indices[query_idx]  # Top N centroids for this query
            top_centroids = set(top_centroids_list)

            # Count vectors to visit for this query
            vectors_to_visit = np.sum(cluster_sizes[top_centroids_list])
            total_vectors_to_visit += vectors_to_visit

            # Count how many ground truth vectors have their assigned centroid in the top N
            found = 0
            for vector_id in gt_vector_ids:
                assigned_centroid = assignments[vector_id]  # Which centroid this vector belongs to
                if assigned_centroid in top_centroids:
                    found += 1

            query_recall = found / len(gt_vector_ids)
            query_recalls.append(query_recall)

        # Compute mean and standard deviation
        average_recall = np.mean(query_recalls)
        std_recall = np.std(query_recalls, ddof=1) if len(query_recalls) > 1 else 0.0
        avg_vectors_to_visit = total_vectors_to_visit / n_queries
        results.append((centroids_to_explore, explore_frac, average_recall, std_recall, avg_vectors_to_visit))

    return results


def print_recall_results(results, knn):
    """Print recall results in a formatted table.

    Args:
        results: List of tuples (centroids_to_explore, explore_fraction, recall_mean, recall_std, avg_vectors_to_visit)
        knn: KNN value used for this result set
    """
    print(f"\n--- Recall@{knn} ---")
    for centroids_to_explore, explore_frac, recall, std_recall, avg_vectors in results:
        print(
            f"Recall@{centroids_to_explore:4d} ({explore_frac * 100:5.2f}% centroids, {avg_vectors:8.0f} avg vectors): {recall:.4f} ± {std_recall:.4f}")


class Timer:
    """Simple timer context manager for measuring execution time."""

    def __init__(self):
        self.start_time = None
        self.elapsed_ms = 0

    def __enter__(self):
        self.start_time = time.perf_counter()
        return self

    def __exit__(self, *args):
        self.elapsed_ms = (time.perf_counter() - self.start_time) * 1000.0

    def get_milliseconds(self):
        return self.elapsed_ms


def write_results_to_csv(
        experiment_name,
        algorithm,
        dataset,
        n_iters,
        actual_iterations,
        dimensionality,
        data_size,
        n_clusters,
        construction_time_ms,
        threads,
        final_objective,
        config_dict,
        results_knn_10,
        results_knn_100
):
    """Write results to CSV file.

    Args:
        experiment_name: Name of the experiment (e.g., "end_to_end")
        algorithm: Name of the algorithm (e.g., "scikit", "fastkmeans")
        dataset: Dataset name
        n_iters: Number of iterations (max requested)
        actual_iterations: Actual iterations performed (may be less if early termination)
        dimensionality: Data dimensionality
        data_size: Number of data points
        n_clusters: Number of clusters
        construction_time_ms: Construction time in milliseconds
        threads: Number of threads used
        final_objective: Final k-means objective value
        config_dict: Dictionary with algorithm-specific configuration (will be serialized to JSON)
        results_knn_10: Results for KNN=10
        results_knn_100: Results for KNN=100
    """
    # Get architecture from environment variable
    arch = os.environ.get('SKM_ARCH', 'default')

    # Get the benchmarks directory (where this script is located)
    benchmarks_dir = Path(__file__).parent

    # Create results directory
    results_dir = benchmarks_dir / 'results' / arch
    results_dir.mkdir(parents=True, exist_ok=True)

    # CSV file path
    csv_path = results_dir / f'{experiment_name}.csv'

    # Check if file exists to determine if we need to write header
    file_exists = csv_path.exists()

    # Get current timestamp
    from datetime import datetime
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Determine if we have recall data
    has_recall_data = len(results_knn_10) > 0 or len(results_knn_100) > 0

    # Prepare header
    header = ['timestamp', 'algorithm', 'dataset', 'n_iters', 'actual_iterations', 'dimensionality',
              'data_size', 'n_clusters', 'construction_time_ms', 'threads', 'final_objective']

    # Add columns for each KNN and explore fraction combination (only if we have recall data)
    if has_recall_data:
        for knn in KNN_VALUES:
            for explore_frac in EXPLORE_FRACTIONS:
                header.append(f'recall@{knn}@{explore_frac * 100:.2f}')
                header.append(f'recall_std@{knn}@{explore_frac * 100:.2f}')
                header.append(f'centroids_explored@{knn}@{explore_frac * 100:.2f}')
                header.append(f'vectors_explored@{knn}@{explore_frac * 100:.2f}')

    header.append('config')

    # Prepare data row
    row = [
        timestamp,
        algorithm,
        dataset,
        n_iters,
        actual_iterations,
        dimensionality,
        data_size,
        n_clusters,
        f'{construction_time_ms:.2f}',
        threads,
        f'{final_objective:.6f}'
    ]

    # Add recall results only if we have data
    if has_recall_data:
        # Add KNN=10 results
        for centroids_to_explore, explore_frac, recall, std_recall, avg_vectors in results_knn_10:
            row.append(f'{recall:.6f}')
            row.append(f'{std_recall:.6f}')
            row.append(str(centroids_to_explore))
            row.append(f'{avg_vectors:.2f}')

        # Add KNN=100 results
        for centroids_to_explore, explore_frac, recall, std_recall, avg_vectors in results_knn_100:
            row.append(f'{recall:.6f}')
            row.append(f'{std_recall:.6f}')
            row.append(str(centroids_to_explore))
            row.append(f'{avg_vectors:.2f}')

    # Add config as JSON string
    config_json = json.dumps(config_dict)
    row.append(config_json)

    # Write to CSV
    with open(csv_path, 'a', newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(header)
        writer.writerow(row)

    print(f"Results written to: {csv_path}")
